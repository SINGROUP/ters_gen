data:
  augmentation:
  - true
  circle_radius: 5
  split_by_id: 0
  train_path: /scratch/phys/sin/sethih1/Extended_TERS_data/planar_oct_2025/planar_npz_1.0_split/train
  val_path: /scratch/phys/sin/sethih1/Extended_TERS_data/planar_oct_2025/planar_npz_1.0_split/val
log_path: /scratch/phys/sin/sethih1/Extended_TERS_data/run_planar_oct_2025/run_logs
model:
  att_channels_options:
  - 16
  - 32
  - 64
  filters_options:
  - - 16
    - 32
    - 64
  - - 16
    - 32
    - 64
    - 128
  - - 16
    - 32
    - 64
    - 128
    - 256
  - - 32
    - 64
    - 128
    - 256
    - 512
  in_channels:
  - 100
  - 400
  kernel_size_options:
  - - 3
    - 3
    - 3
  - - 3
    - 3
    - 3
    - 3
  - - 3
    - 3
    - 3
    - 3
    - 3
  - - 3
    - 3
    - 3
    - 3
    - 3
    - 3
  out_channels: 1
  type: AttentionUNet
save_path: /scratch/phys/sin/sethih1/Extended_TERS_data/run_planar_oct_2025/models
training:
  batch_sizes:
  - 16
  - 32
  - 64
  - 128
  epochs: 50
  gpu_ids:
  - 0
  learning_rates:
  - 1.0e-05
  - 0.001
  loss_functions:
  - dice_loss
  n_jobs: 1
  n_trials: 4
  use_parallel: true

[I 2025-10-15 15:57:56,877] A new study created in memory with name: no-name-f81d328a-0402-479e-87c3-664831b0df2a
wandb: Currently logged in as: sethih10 (sethih10-epfl) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /home/sethih1/masque_new/ters_gen/wandb/run-20251015_155757-dm34awmu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_1_bs64_lr2e-04_dice_loss
wandb: ‚≠êÔ∏è View project at https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32
wandb: üöÄ View run at https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32/runs/dm34awmu
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /home/sethih1/masque_new/ters_gen/wandb/run-20251015_155802-3ei0dnzb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_2_bs128_lr1e-04_dice_loss
wandb: ‚≠êÔ∏è View project at https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32
wandb: üöÄ View run at https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32/runs/3ei0dnzb
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /home/sethih1/masque_new/ters_gen/wandb/run-20251015_155802-4o7bhodc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_0_bs128_lr8e-04_dice_loss
wandb: ‚≠êÔ∏è View project at https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32
wandb: üöÄ View run at https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32/runs/4o7bhodc
wandb: creating run
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /home/sethih1/masque_new/ters_gen/wandb/run-20251015_155757-7toupdq5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_3_bs64_lr7e-05_dice_loss
wandb: ‚≠êÔ∏è View project at https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32
wandb: üöÄ View run at https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32/runs/7toupdq5
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 80, in objective
    model = get_model(config.model.type, model_params).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 80, in objective
    model = get_model(config.model.type, model_params).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 80, in objective
    model = get_model(config.model.type, model_params).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
           ^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
           ^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
           ^^^^^
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

wandb: uploading wandb-metadata.json; uploading requirements.txt; uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading requirements.txt; uploading wandb-metadata.json
wandb: uploading wandb-metadata.json; uploading requirements.txt; uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: üöÄ View run trial_2_bs128_lr1e-04_dice_loss at: https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32/runs/3ei0dnzb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251015_155802-3ei0dnzb/logs
[I 2025-10-15 15:58:06,055] Trial 2 finished with value: 0.0 and parameters: {'batch_size': 128, 'lr': 0.00014476357976727516, 'loss_fn': 'dice_loss', 'augmentation': True, 'filters_idx': 2, 'in_channels': 400, 'att_channels': 64}. Best is trial 2 with value: 0.0.
wandb: uploading summary, console lines 5-76
wandb:                                                                                
wandb: üöÄ View run trial_0_bs128_lr8e-04_dice_loss at: https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32/runs/4o7bhodc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251015_155802-4o7bhodc/logs
[I 2025-10-15 15:58:06,241] Trial 0 finished with value: 0.0 and parameters: {'batch_size': 128, 'lr': 0.0008203702800366896, 'loss_fn': 'dice_loss', 'augmentation': True, 'filters_idx': 1, 'in_channels': 100, 'att_channels': 16}. Best is trial 2 with value: 0.0.
wandb: uploading history steps 0-2, summary, console lines 3-70
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: final_dice ‚ñÅ‚ñÅ‚ñÅ
wandb:      trial ‚ñÅ‚ñÜ‚ñà
wandb: 
wandb: Run summary:
wandb: final_dice 0
wandb:      trial 3
wandb: 
wandb: üöÄ View run trial_3_bs64_lr7e-05_dice_loss at: https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32/runs/7toupdq5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251015_155757-7toupdq5/logs
[I 2025-10-15 15:58:10,305] Trial 3 finished with value: 0.0 and parameters: {'batch_size': 64, 'lr': 6.984994379262919e-05, 'loss_fn': 'dice_loss', 'augmentation': True, 'filters_idx': 2, 'in_channels': 400, 'att_channels': 16}. Best is trial 2 with value: 0.0.
Exception in trial 3: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Exception in trial 0: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Exception in trial 2: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.



Epoch    1/50, time: 350.78 s, training_loss: 0.875
Epoch    1/50, time: 44.59 s, val_loss: 0.848
Training Metrics:
Accuracy: 0.8885
Precision: 0.1245
Recall: 0.8889
F1 Score: 0.2184
IoU: 0.1226
Dice Coefficient: 0.2184
Validation Metrics:
Accuracy: 0.8848
Precision: 0.1207
Recall: 0.8970
F1 Score: 0.2128
IoU: 0.1191
Dice Coefficient: 0.2128
Epoch    2/50, time: 185.70 s, training_loss: 0.834
Epoch    2/50, time: 21.36 s, val_loss: 0.817
Epoch    3/50, time: 178.89 s, training_loss: 0.815
Epoch    3/50, time: 24.50 s, val_loss: 0.801
Epoch    4/50, time: 203.62 s, training_loss: 0.797
Epoch    4/50, time: 21.43 s, val_loss: 0.788
Epoch    5/50, time: 177.75 s, training_loss: 0.781
Epoch    5/50, time: 21.53 s, val_loss: 0.774
Epoch    6/50, time: 176.90 s, training_loss: 0.766
Epoch    6/50, time: 21.77 s, val_loss: 0.755
Epoch    7/50, time: 179.47 s, training_loss: 0.751
Epoch    7/50, time: 21.19 s, val_loss: 0.752
Epoch    8/50, time: 179.87 s, training_loss: 0.732
Epoch    8/50, time: 23.26 s, val_loss: 0.729
Epoch    9/50, time: 181.37 s, training_loss: 0.718
Epoch    9/50, time: 21.53 s, val_loss: 0.716
Epoch   10/50, time: 183.18 s, training_loss: 0.705
Epoch   10/50, time: 21.22 s, val_loss: 0.715
Epoch   11/50, time: 182.53 s, training_loss: 0.692
Epoch   11/50, time: 21.30 s, val_loss: 0.701
Training Metrics:
Accuracy: 0.9514
Precision: 0.2092
Recall: 0.6373
F1 Score: 0.3150
IoU: 0.1870
Dice Coefficient: 0.3150
Validation Metrics:
Accuracy: 0.9488
Precision: 0.1976
Recall: 0.6366
F1 Score: 0.3015
IoU: 0.1775
Dice Coefficient: 0.3015
Epoch   12/50, time: 183.17 s, training_loss: 0.681
Epoch   12/50, time: 21.79 s, val_loss: 0.707
Epoch   13/50, time: 218.67 s, training_loss: 0.670
Epoch   13/50, time: 25.94 s, val_loss: 0.670
Epoch   14/50, time: 179.62 s, training_loss: 0.662
Epoch   14/50, time: 22.02 s, val_loss: 0.674
Epoch   15/50, time: 184.30 s, training_loss: 0.654
Epoch   15/50, time: 23.83 s, val_loss: 0.653
Epoch   16/50, time: 177.37 s, training_loss: 0.645
Epoch   16/50, time: 23.08 s, val_loss: 0.650
Epoch   17/50, time: 179.31 s, training_loss: 0.639
Epoch   17/50, time: 23.48 s, val_loss: 0.670
Epoch   18/50, time: 178.70 s, training_loss: 0.633
Epoch   18/50, time: 21.55 s, val_loss: 0.662
Epoch   19/50, time: 173.21 s, training_loss: 0.629
Epoch   19/50, time: 23.86 s, val_loss: 0.636
Epoch   20/50, time: 207.39 s, training_loss: 0.623
Epoch   20/50, time: 22.15 s, val_loss: 0.637
Epoch   21/50, time: 203.06 s, training_loss: 0.621
Epoch   21/50, time: 22.65 s, val_loss: 0.627
Training Metrics:
Accuracy: 0.9667
Precision: 0.2887
Recall: 0.6137
F1 Score: 0.3927
IoU: 0.2443
Dice Coefficient: 0.3927
Validation Metrics:
Accuracy: 0.9656
Precision: 0.2744
Recall: 0.5963
F1 Score: 0.3759
IoU: 0.2314
Dice Coefficient: 0.3759
Epoch   22/50, time: 180.78 s, training_loss: 0.615
Epoch   22/50, time: 25.69 s, val_loss: 0.650
Epoch   23/50, time: 182.60 s, training_loss: 0.611
Epoch   23/50, time: 23.58 s, val_loss: 0.613
Epoch   24/50, time: 209.13 s, training_loss: 0.608
Epoch   24/50, time: 21.68 s, val_loss: 0.618
Epoch   25/50, time: 210.35 s, training_loss: 0.603
Epoch   25/50, time: 21.91 s, val_loss: 0.609
Epoch   26/50, time: 177.57 s, training_loss: 0.599
Epoch   26/50, time: 23.87 s, val_loss: 0.631
Epoch   27/50, time: 203.60 s, training_loss: 0.599
Epoch   27/50, time: 21.11 s, val_loss: 0.615
Epoch   28/50, time: 173.63 s, training_loss: 0.596
Epoch   28/50, time: 21.01 s, val_loss: 0.621
Epoch   29/50, time: 210.77 s, training_loss: 0.594
Epoch   29/50, time: 20.69 s, val_loss: 0.608
Epoch   30/50, time: 173.15 s, training_loss: 0.591
Epoch   30/50, time: 21.20 s, val_loss: 0.616
Epoch   31/50, time: 174.95 s, training_loss: 0.591
Epoch   31/50, time: 21.28 s, val_loss: 0.612
Training Metrics:
Accuracy: 0.9726
Precision: 0.3323
Recall: 0.5588
F1 Score: 0.4168
IoU: 0.2632
Dice Coefficient: 0.4168
Validation Metrics:
Accuracy: 0.9724
Precision: 0.3256
Recall: 0.5502
F1 Score: 0.4091
IoU: 0.2571
Dice Coefficient: 0.4091
Epoch   32/50, time: 178.85 s, training_loss: 0.585
Epoch   32/50, time: 21.78 s, val_loss: 0.601
Epoch   33/50, time: 181.75 s, training_loss: 0.585
Epoch   33/50, time: 21.92 s, val_loss: 0.598
Epoch   34/50, time: 183.48 s, training_loss: 0.584
Epoch   34/50, time: 22.76 s, val_loss: 0.599
Epoch   35/50, time: 182.43 s, training_loss: 0.579
Epoch   35/50, time: 22.04 s, val_loss: 0.603
Epoch   36/50, time: 179.16 s, training_loss: 0.575
Epoch   36/50, time: 21.34 s, val_loss: 0.586
Epoch   37/50, time: 180.04 s, training_loss: 0.574
Epoch   37/50, time: 21.38 s, val_loss: 0.593
Epoch   38/50, time: 212.38 s, training_loss: 0.573
Epoch   38/50, time: 21.92 s, val_loss: 0.590
Epoch   39/50, time: 180.22 s, training_loss: 0.574
Epoch   39/50, time: 22.96 s, val_loss: 0.613
Epoch   40/50, time: 179.15 s, training_loss: 0.573
Epoch   40/50, time: 22.48 s, val_loss: 0.596
Epoch   41/50, time: 176.57 s, training_loss: 0.568
Epoch   41/50, time: 29.07 s, val_loss: 0.599
Training Metrics:
Accuracy: 0.9736
Precision: 0.3480
Recall: 0.5768
F1 Score: 0.4341
IoU: 0.2772
Dice Coefficient: 0.4341
Validation Metrics:
Accuracy: 0.9734
Precision: 0.3394
Recall: 0.5648
F1 Score: 0.4240
IoU: 0.2690
Dice Coefficient: 0.4240
Epoch   42/50, time: 182.72 s, training_loss: 0.566
Epoch   42/50, time: 21.36 s, val_loss: 0.590
Epoch   43/50, time: 180.57 s, training_loss: 0.566
Epoch   43/50, time: 21.01 s, val_loss: 0.590
Epoch   44/50, time: 171.81 s, training_loss: 0.565
Epoch   44/50, time: 20.66 s, val_loss: 0.579
Epoch   45/50, time: 174.20 s, training_loss: 0.563
Epoch   45/50, time: 21.97 s, val_loss: 0.581
Epoch   46/50, time: 175.00 s, training_loss: 0.561
Epoch   46/50, time: 22.56 s, val_loss: 0.581
Epoch   47/50, time: 176.31 s, training_loss: 0.564
Epoch   47/50, time: 26.15 s, val_loss: 0.587
Epoch   48/50, time: 173.88 s, training_loss: 0.558
Epoch   48/50, time: 21.80 s, val_loss: 0.576
Epoch   49/50, time: 201.84 s, training_loss: 0.560
Epoch   49/50, time: 20.44 s, val_loss: 0.581
Epoch   50/50, time: 177.99 s, training_loss: 0.558
Epoch   50/50, time: 21.02 s, val_loss: 0.576
Training Metrics:
Accuracy: 0.9694
Precision: 0.3226
Recall: 0.6761
F1 Score: 0.4368
IoU: 0.2794
Dice Coefficient: 0.4368
Validation Metrics:
Accuracy: 0.9698
Precision: 0.3187
Recall: 0.6508
F1 Score: 0.4279
IoU: 0.2721
Dice Coefficient: 0.4279
[W 2025-10-15 19:21:01,141] Trial 1 failed with parameters: {'batch_size': 64, 'lr': 0.0001659100267364182, 'loss_fn': 'dice_loss', 'augmentation': True, 'filters_idx': 1, 'in_channels': 100, 'att_channels': 64} because of the following error: Error('You must call wandb.init() before wandb.log()').
Traceback (most recent call last):
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/study/_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 171, in <lambda>
    lambda t: objective(t, config, gpu_queue, args.use_wandb),
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 132, in objective
    wandb.log({"final_dice": final_dice, "trial": trial.number})
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/wandb/sdk/lib/preinit.py", line 36, in preinit_wrapper
    raise wandb.Error(f"You must call wandb.init() before {name}()")
wandb.errors.errors.Error: You must call wandb.init() before wandb.log()
[W 2025-10-15 19:21:01,143] Trial 1 failed with value None.
Test Metrics:
Accuracy: 0.9698
Precision: 0.3187
Recall: 0.6508
F1 Score: 0.4279
IoU: 0.2721
Dice Coefficient: 0.4279
Optuna results saving
Trials logged to /scratch/phys/sin/sethih1/Extended_TERS_data/run_planar_oct_2025/run_logs/optuna_trials.csv
Best params: {'batch_size': 128, 'lr': 0.00014476357976727516, 'loss_fn': 'dice_loss', 'augmentation': True, 'filters_idx': 2, 'in_channels': 400, 'att_channels': 64}
Best dice: 0.0
Traceback (most recent call last):
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 195, in <module>
    main()
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 192, in main
    visualize_study(study, os.path.join(config.log_path, "visualizations"))
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 149, in visualize_study
    plot_param_importances(study).write_html(os.path.join(output_dir, "param_importances.html"))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/visualization/_param_importances.py", line 168, in plot_param_importances
    importances_infos = _get_importances_infos(study, evaluator, params, target, target_name)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/visualization/_param_importances.py", line 82, in _get_importances_infos
    _get_importances_info(
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/visualization/_param_importances.py", line 54, in _get_importances_info
    importances = optuna.importance.get_param_importances(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/importance/__init__.py", line 111, in get_param_importances
    res = evaluator.evaluate(study, params=params, target=target)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/importance/_fanova/_evaluator.py", line 117, in evaluate
    evaluator.fit(
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/importance/_fanova/_fanova.py", line 73, in fit
    raise RuntimeError("Encountered zero total variance in all trees.")
RuntimeError: Encountered zero total variance in all trees.
Traceback (most recent call last):
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 195, in <module>
    main()
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 192, in main
    visualize_study(study, os.path.join(config.log_path, "visualizations"))
  File "/home/sethih1/masque_new/ters_gen/hyperopt.py", line 149, in visualize_study
    plot_param_importances(study).write_html(os.path.join(output_dir, "param_importances.html"))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/visualization/_param_importances.py", line 168, in plot_param_importances
    importances_infos = _get_importances_infos(study, evaluator, params, target, target_name)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/visualization/_param_importances.py", line 82, in _get_importances_infos
    _get_importances_info(
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/visualization/_param_importances.py", line 54, in _get_importances_info
    importances = optuna.importance.get_param_importances(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/importance/__init__.py", line 111, in get_param_importances
    res = evaluator.evaluate(study, params=params, target=target)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/importance/_fanova/_evaluator.py", line 117, in evaluate
    evaluator.fit(
  File "/scratch/phys/sin/sethih1/venv/masque_env/lib/python3.11/site-packages/optuna/importance/_fanova/_fanova.py", line 73, in fit
    raise RuntimeError("Encountered zero total variance in all trees.")
RuntimeError: Encountered zero total variance in all trees.
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mtrial_1_bs64_lr2e-04_dice_loss[0m at: [34mhttps://wandb.ai/sethih10-epfl/Posnet_50epochs_just_aug_val_32x32/runs/dm34awmu[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251015_155757-dm34awmu/logs[0m
